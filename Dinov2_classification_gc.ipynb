{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LELzIUsVB2NS"
      },
      "source": [
        "![Roboflow Notebooks banner](https://camo.githubusercontent.com/aec53c2b5fb6ed43d202a0ab622b58ba68a89d654fbe3abab0c0cc8bd1ff424e/68747470733a2f2f696b2e696d6167656b69742e696f2f726f626f666c6f772f6e6f7465626f6f6b732f74656d706c6174652f62616e6e657274657374322d322e706e673f696b2d73646b2d76657273696f6e3d6a6176617363726970742d312e342e33267570646174656441743d31363732393332373130313934)\n",
        "\n",
        "# Image Classification with DINOv2\n",
        "\n",
        "DINOv2, released by Meta Research in April 2023, implements a self-supervised method of training computer vision models.\n",
        "\n",
        "DINOv2 was trained using 140 million images without labels. The embeddings generated by DINOv2 can be used for classification, image retrieval, segmentation, and depth estimation. With that said, Meta Research did not release heads for segmentation and depth estimation.\n",
        "In this guide, we are going to build an image classifier using embeddings from DINOv2. To do so, we will:\n",
        "\n",
        "1. Load a folder of images\n",
        "2. Compute embeddings for each image\n",
        "3. Save all the embeddings in a file and vector store\n",
        "4. Train an SVM classifier to classify images\n",
        "\n",
        "By the end of this notebook, we'll have a classifier trained on our dataset.\n",
        "\n",
        "Without further ado, let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82ae19kDpbW"
      },
      "source": [
        "## Import Packages\n",
        "\n",
        "First, let's import the packages we will need for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7yEBuTRyo5e3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import os\n",
        "#import cv2\n",
        "import json\n",
        "import glob\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BPT7w3mEKrf"
      },
      "outputs": [],
      "source": [
        "import roboflow\n",
        "#import supervision as sv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspaces/gc_quant_trading_research'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cwd = os.getcwd()\n",
        "cwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load folder containing the trading images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "064ynDDhCU2i",
        "outputId": "a37c0f2c-f8fa-48e1-876e-411ce4cd7015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2D_embedding_viz.ipynb\n",
            "all_embeddings.json\n",
            "bear\n",
            "range\n",
            "# %% [markdown]\n",
            "now\n",
            "Dinov2_classification_gc.ipynb\n",
            "2d_scatter_plot_4candles.html\n",
            "2d_scatter_plot.html\n",
            "bull\n",
            "README.md\n",
            "dockerfile\n",
            "notebooks\n",
            ".git\n",
            "data\n",
            "Now\n",
            "requirements.txt\n"
          ]
        }
      ],
      "source": [
        "cwd = os.getcwd()\n",
        "\n",
        "ROOT_DIR = os.path.join(cwd)\n",
        "\n",
        "labels = {}\n",
        "\n",
        "for folder in os.listdir(ROOT_DIR):\n",
        "  try:\n",
        "    print(folder)\n",
        "    for file in os.listdir(os.path.join(ROOT_DIR, folder)):\n",
        "        if file.endswith(\".png\"):\n",
        "            full_name = os.path.join(ROOT_DIR, folder, file)\n",
        "            labels[full_name] = folder\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "files = labels.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWaTpa_rtqHo",
        "outputId": "3181a50d-b351-4467-9a14-5acf6c428e5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.18.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.56.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.06.59.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.44.35.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.23.25.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.07.55.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.24.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.07.43.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.43.53.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.49.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.15.57.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.14.53.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.41.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.44.12.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.39.40.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.16.11.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.15.03.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.44.03.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.44.28.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.25.49.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.14.12.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.15.25.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.25.25.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.41.18.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.41.46.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.48.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.43.41.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.26.39.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.15.46.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.14.30.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.14.21.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.14.39.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.26.25.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.30.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.26.52.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.35.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.41.03.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.07.20.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.08.37.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.07.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.07.49.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.16.04.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.40.40.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.44.22.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.26.57.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.45.10.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.41.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.06.49.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.26.07.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.10.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.23.11.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.06.37.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.06.17.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.41.40.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.41.52.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.07.36.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.06.26.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 12.08.15.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.27.17.png',\n",
              " '/workspaces/gc_quant_trading_research/bear/Screenshot 2024-09-17 at 11.42.01.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.48.12.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.24.53.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.13.17.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.13.30.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.11.53.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.33.31.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.40.14.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.18.16.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.47.54.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.43.09.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.18.26.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.24.17.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.12.02.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.12.25.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 12.18.34.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.40.52.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.48.02.png',\n",
              " '/workspaces/gc_quant_trading_research/range/Screenshot 2024-09-17 at 11.40.08.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.00.10.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.32.20.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.45.40.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.36.51.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.11.07.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.12.59.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.29.27.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.29.55.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.35.27.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.11.17.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.18.01.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.38.15.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.50.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.37.53.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.16.42.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.25.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.17.38.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.00.03.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.00.27.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.17.11.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.36.13.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.40.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.59.30.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.38.32.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.00.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.44.49.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.36.03.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.13.43.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.59.19.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.37.31.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.12.42.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.36.22.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.31.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.12.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.35.38.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.37.19.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.28.34.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.16.29.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.45.33.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.32.32.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.45.23.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.31.55.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.36.59.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.12.51.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.38.24.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.59.39.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.59.09.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.16.59.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.13.04.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.31.28.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.35.48.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.29.41.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.16.36.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.00.18.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.59.50.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.17.54.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.46.07.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.47.13.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.28.18.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.11.29.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.16.52.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.47.31.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.47.39.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 12.17.24.png',\n",
              " '/workspaces/gc_quant_trading_research/bull/Screenshot 2024-09-17 at 11.45.54.png']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "b2zHBl9CP9pR"
      },
      "outputs": [],
      "source": [
        "# prompt: get data from dictionary files\n",
        "\n",
        "values = [labels[key] for key in files]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1NvdxMOEUnH"
      },
      "source": [
        "## Load the Model and Compute Embeddings\n",
        "\n",
        "To train our classifier, we need:\n",
        "\n",
        "1. The embeddings associated with each image in our dataset, and;\n",
        "2. The labels associated with each image.\n",
        "\n",
        "To calculate embeddings, we'll use DINOv2. Below, we load the smallest DINOv2 weights and define functions that will load and compute embeddings for every image in a specified list.\n",
        "\n",
        "We store all of our vectors in a dictionary that is saved to disk so we can reference them again if needed. Note that in production environments one may opt for using another data structure such as a vector embedding database (i.e. faiss) for storing embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF7rqN44CVLV",
        "outputId": "f8427a1b-9e22-46f1-c124-422c61b30590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /home/codespace/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "xFormers is not available (SwiGLU)\n",
            "xFormers is not available (Attention)\n",
            "xFormers is not available (Block)\n"
          ]
        }
      ],
      "source": [
        "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dinov2_vits14.to(device)\n",
        "\n",
        "transform_image = T.Compose([T.ToTensor(),\n",
        "                             T.Resize((70, 210)),\n",
        "                             #T.CenterCrop(224),\n",
        "                             T.Normalize([0.5], [0.5])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T0woC0J7CX-z"
      },
      "outputs": [],
      "source": [
        "def load_image(img: str) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Load an image and return a tensor that can be used as an input to DINOv2.\n",
        "    \"\"\"\n",
        "    img = Image.open(img)\n",
        "\n",
        "    transformed_img = transform_image(img)[:3].unsqueeze(0)\n",
        "\n",
        "    return transformed_img\n",
        "\n",
        "def compute_embeddings(files: list) -> dict:\n",
        "    \"\"\"\n",
        "    Create an index that contains all of the images in the specified list of files.\n",
        "    \"\"\"\n",
        "    all_embeddings = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, file in enumerate(files):\n",
        "        embeddings = dinov2_vits14(load_image(file).to(device))\n",
        "\n",
        "        all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n",
        "\n",
        "    with open(\"all_embeddings.json\", \"w\") as f:\n",
        "        f.write(json.dumps(all_embeddings))\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTqbdE17FIGl"
      },
      "source": [
        "## Compute Embeddings\n",
        "\n",
        "The code below computes the embeddings for all the images in our dataset. This step will take a few minutes for the MIT Indoor Scene Recognition dataset. There are over 10,000 images in the training set that we need to pass through DINOv2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "30738bf62c8a46c3912d1249b2386060",
            "a6c564ccbc4c4fb6845f93fbe2091926",
            "9691ec7d7f85479eb038354efde083ff",
            "18fa966614f944ecb0177de7356c7b71",
            "90b0c891c99a42fd81dfebd2120695ca",
            "cc0824e57c954bd3a32c6332552b6c7a",
            "e9ba7e7d40f2407488ad9b7f1822a67c",
            "b452c76402b04833961649b4a351ba5b",
            "c05a21787df74ac7aa43a5e7bb949238",
            "de9b5632c5af4c5abf413aad5ea25eca",
            "5d9767e2157f40cebf8a3f3994959d5f"
          ]
        },
        "id": "WDMx6fBVCqib",
        "outputId": "2ff5d21c-5456-4795-8389-b2786f8e01a9"
      },
      "outputs": [],
      "source": [
        "embeddings = compute_embeddings(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ef2iJ-iLRmpU"
      },
      "outputs": [],
      "source": [
        "embedding_list = list(embeddings.values())\n",
        "embedding_arr = np.array(embedding_list).reshape(-1, 384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.16968729,  0.76022828, -3.43099904, ..., -2.73236132,\n",
              "        -1.99714077,  2.20648551],\n",
              "       [-2.34796762,  2.33557105, -2.40413761, ..., -2.73249078,\n",
              "         0.36726138,  3.9965446 ],\n",
              "       [-2.96414304,  1.35418332, -4.17845964, ..., -3.48992133,\n",
              "        -1.29128671,  4.04121876],\n",
              "       ...,\n",
              "       [-3.25267744,  1.70402408, -3.62107205, ..., -1.81900656,\n",
              "         0.38233238,  2.65884495],\n",
              "       [-3.31155705,  2.10577202, -3.62780905, ..., -2.59082246,\n",
              "        -0.26570469,  3.09453702],\n",
              "       [-2.79943347,  1.57769716, -4.10529995, ..., -3.56242919,\n",
              "        -2.37703991,  3.9444232 ]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK-mPpkuFWJX"
      },
      "source": [
        "## Train a Classification Model\n",
        "\n",
        "The embeddings we have computed can be used as an input in a classification model. For this guide, we will be using SVM, a linear classification model.\n",
        "\n",
        "Below, we make lists of both all of the embeddings we have computed and their associated labels. We then fit our model using those lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "tHrpE_-pCu9z",
        "outputId": "8d86ad8e-fb55-43d6-d910-98c59b2488d0"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#fit a svm model\n",
        "def fit_svm(embeddings, labels):\n",
        "    clf = svm.SVC(gamma='scale')\n",
        "    y = [labels[file] for file in files]\n",
        "    embedding_list = list(embeddings.values())\n",
        "    clf.fit(np.array(embedding_list).reshape(-1, 384), y)\n",
        "    return clf\n",
        "\n",
        "clf_svm = fit_svm(embeddings, labels)\n",
        "\n",
        "\n",
        "#fit a random forest model\n",
        "def fit_rf(embeddings, labels):\n",
        "    rf = RandomForestClassifier(n_estimators=1000)\n",
        "    y = [labels[file] for file in files]\n",
        "    embedding_list = list(embeddings.values())\n",
        "    rf.fit(np.array(embedding_list).reshape(-1, 384), y)\n",
        "    return rf\n",
        "\n",
        "clf_rf = fit_rf(embeddings, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOytKjPdFqwD"
      },
      "source": [
        "## Classify an Image\n",
        "\n",
        "We now have a classifier we can use to classify images!\n",
        "\n",
        "Change the `input_file` value below to the path of a file in the `valid` or `test` directories in the image dataset with which we have been working.\n",
        "\n",
        "Then, run the cell to classify the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "peRLdL1DC5lN",
        "outputId": "43cb02b8-dd72-4939-af6a-108b799a54ac"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "#any file in the folder title Now\n",
        "input_files = glob.glob(\"Now/*.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now/Screenshot 2024-09-17 at 12.31.20.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.30.07.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.30.57.png\n",
            "\n",
            "SVM Predicted class: bear\n",
            "RF Predicted class: bear\n",
            "Now/Screenshot 2024-09-17 at 12.29.18.png\n",
            "\n",
            "SVM Predicted class: bear\n",
            "RF Predicted class: bear\n",
            "Now/Screenshot 2024-09-17 at 12.30.18.png\n",
            "\n",
            "SVM Predicted class: bear\n",
            "RF Predicted class: bear\n",
            "Now/Screenshot 2024-09-17 at 12.29.35.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.29.44.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.31.05.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.30.43.png\n",
            "\n",
            "SVM Predicted class: bear\n",
            "RF Predicted class: bear\n",
            "Now/Screenshot 2024-09-17 at 12.31.33.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.29.25.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bull\n",
            "Now/Screenshot 2024-09-17 at 12.30.30.png\n",
            "\n",
            "SVM Predicted class: bear\n",
            "RF Predicted class: bear\n",
            "Now/Screenshot 2024-09-17 at 12.30.38.png\n",
            "\n",
            "SVM Predicted class: bull\n",
            "RF Predicted class: bear\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_file</th>\n",
              "      <th>svm_prediction</th>\n",
              "      <th>rf_prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.31.20.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.07.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.57.png</td>\n",
              "      <td>bear</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.29.18.png</td>\n",
              "      <td>bear</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.18.png</td>\n",
              "      <td>bear</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.29.35.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.29.44.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.31.05.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.43.png</td>\n",
              "      <td>bear</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.31.33.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.29.25.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.30.png</td>\n",
              "      <td>bear</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Now/Screenshot 2024-09-17 at 12.30.38.png</td>\n",
              "      <td>bull</td>\n",
              "      <td>bear</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   image_file svm_prediction rf_prediction\n",
              "0   Now/Screenshot 2024-09-17 at 12.31.20.png           bull          bull\n",
              "1   Now/Screenshot 2024-09-17 at 12.30.07.png           bull          bull\n",
              "2   Now/Screenshot 2024-09-17 at 12.30.57.png           bear          bear\n",
              "3   Now/Screenshot 2024-09-17 at 12.29.18.png           bear          bear\n",
              "4   Now/Screenshot 2024-09-17 at 12.30.18.png           bear          bear\n",
              "5   Now/Screenshot 2024-09-17 at 12.29.35.png           bull          bull\n",
              "6   Now/Screenshot 2024-09-17 at 12.29.44.png           bull          bull\n",
              "7   Now/Screenshot 2024-09-17 at 12.31.05.png           bull          bull\n",
              "8   Now/Screenshot 2024-09-17 at 12.30.43.png           bear          bear\n",
              "9   Now/Screenshot 2024-09-17 at 12.31.33.png           bull          bull\n",
              "10  Now/Screenshot 2024-09-17 at 12.29.25.png           bull          bull\n",
              "11  Now/Screenshot 2024-09-17 at 12.30.30.png           bear          bear\n",
              "12  Now/Screenshot 2024-09-17 at 12.30.38.png           bull          bear"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "predictions_df = pd.DataFrame(columns=['image_file', 'svm_prediction', 'rf_prediction'])\n",
        "\n",
        "\n",
        "\n",
        "for input_file in input_files:\n",
        "    new_image = load_image(input_file)\n",
        "    print(input_file)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding = dinov2_vits14(new_image.to(device))\n",
        "        \n",
        "        # Generate predictions\n",
        "        svm_prediction = clf_svm.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
        "        rf_prediction = clf_rf.predict(np.array(embedding[0].cpu()).reshape(1, -1))\n",
        "\n",
        "        # Add the predictions to the DataFrame using loc\n",
        "        predictions_df.loc[len(predictions_df)] = [input_file, svm_prediction[0], rf_prediction[0]]\n",
        "\n",
        "        print()\n",
        "        print(\"SVM Predicted class: \" + svm_prediction[0])\n",
        "        print(\"RF Predicted class: \" + rf_prediction[0])\n",
        "\n",
        "# Print the DataFrame\n",
        "predictions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18fa966614f944ecb0177de7356c7b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de9b5632c5af4c5abf413aad5ea25eca",
            "placeholder": "​",
            "style": "IPY_MODEL_5d9767e2157f40cebf8a3f3994959d5f",
            "value": " 218/218 [01:26&lt;00:00,  3.93it/s]"
          }
        },
        "30738bf62c8a46c3912d1249b2386060": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6c564ccbc4c4fb6845f93fbe2091926",
              "IPY_MODEL_9691ec7d7f85479eb038354efde083ff",
              "IPY_MODEL_18fa966614f944ecb0177de7356c7b71"
            ],
            "layout": "IPY_MODEL_90b0c891c99a42fd81dfebd2120695ca"
          }
        },
        "5d9767e2157f40cebf8a3f3994959d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90b0c891c99a42fd81dfebd2120695ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9691ec7d7f85479eb038354efde083ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b452c76402b04833961649b4a351ba5b",
            "max": 218,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c05a21787df74ac7aa43a5e7bb949238",
            "value": 218
          }
        },
        "a6c564ccbc4c4fb6845f93fbe2091926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc0824e57c954bd3a32c6332552b6c7a",
            "placeholder": "​",
            "style": "IPY_MODEL_e9ba7e7d40f2407488ad9b7f1822a67c",
            "value": "100%"
          }
        },
        "b452c76402b04833961649b4a351ba5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c05a21787df74ac7aa43a5e7bb949238": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc0824e57c954bd3a32c6332552b6c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9b5632c5af4c5abf413aad5ea25eca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9ba7e7d40f2407488ad9b7f1822a67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
