# %% [markdown]
# ![Roboflow Notebooks banner](https://camo.githubusercontent.com/aec53c2b5fb6ed43d202a0ab622b58ba68a89d654fbe3abab0c0cc8bd1ff424e/68747470733a2f2f696b2e696d6167656b69742e696f2f726f626f666c6f772f6e6f7465626f6f6b732f74656d706c6174652f62616e6e657274657374322d322e706e673f696b2d73646b2d76657273696f6e3d6a6176617363726970742d312e342e33267570646174656441743d31363732393332373130313934)
# 
# # Image Classification with DINOv2
# 
# DINOv2, released by Meta Research in April 2023, implements a self-supervised method of training computer vision models.
# 
# DINOv2 was trained using 140 million images without labels. The embeddings generated by DINOv2 can be used for classification, image retrieval, segmentation, and depth estimation. With that said, Meta Research did not release heads for segmentation and depth estimation.
# In this guide, we are going to build an image classifier using embeddings from DINOv2. To do so, we will:
# 
# 1. Load a folder of images
# 2. Compute embeddings for each image
# 3. Save all the embeddings in a file and vector store
# 4. Train an SVM classifier to classify images
# 
# By the end of this notebook, we'll have a classifier trained on our dataset.
# 
# Without further ado, let's begin!

# %% [markdown]
# ## Import Packages
# 
# First, let's import the packages we will need for this project.

# %%
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
import os
#import cv2
import json
import glob
from tqdm.notebook import tqdm

# %%
import roboflow
#import supervision as sv


# %%
cwd = os.getcwd()
cwd

# %% [markdown]
# Load folder containing the trading images

# %%
cwd = os.getcwd()

ROOT_DIR = os.path.join(cwd)

labels = {}

for folder in os.listdir(ROOT_DIR):
  try:
    print(folder)
    for file in os.listdir(os.path.join(ROOT_DIR, folder)):
        if file.endswith(".png"):
            full_name = os.path.join(ROOT_DIR, folder, file)
            labels[full_name] = folder
  except:
    pass

files = labels.keys()

# %%
list(files)

# %%
# prompt: get data from dictionary files

values = [labels[key] for key in files]

# %% [markdown]
# ## Load the Model and Compute Embeddings
# 
# To train our classifier, we need:
# 
# 1. The embeddings associated with each image in our dataset, and;
# 2. The labels associated with each image.
# 
# To calculate embeddings, we'll use DINOv2. Below, we load the smallest DINOv2 weights and define functions that will load and compute embeddings for every image in a specified list.
# 
# We store all of our vectors in a dictionary that is saved to disk so we can reference them again if needed. Note that in production environments one may opt for using another data structure such as a vector embedding database (i.e. faiss) for storing embeddings.

# %%
dinov2_vits14 = torch.hub.load("facebookresearch/dinov2", "dinov2_vits14")

device = torch.device('cuda' if torch.cuda.is_available() else "cpu")

dinov2_vits14.to(device)

transform_image = T.Compose([T.ToTensor(),
                             T.Resize((70, 210)),
                             #T.CenterCrop(224),
                             T.Normalize([0.5], [0.5])])

# %%
def load_image(img: str) -> torch.Tensor:
    """
    Load an image and return a tensor that can be used as an input to DINOv2.
    """
    img = Image.open(img)

    transformed_img = transform_image(img)[:3].unsqueeze(0)

    return transformed_img

def compute_embeddings(files: list) -> dict:
    """
    Create an index that contains all of the images in the specified list of files.
    """
    all_embeddings = {}

    with torch.no_grad():
      for i, file in enumerate(files):
        embeddings = dinov2_vits14(load_image(file).to(device))

        all_embeddings[file] = np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()

    with open("all_embeddings.json", "w") as f:
        f.write(json.dumps(all_embeddings))

    return all_embeddings

# %% [markdown]
# ## Compute Embeddings
# 
# The code below computes the embeddings for all the images in our dataset. This step will take a few minutes for the MIT Indoor Scene Recognition dataset. There are over 10,000 images in the training set that we need to pass through DINOv2.

# %%
embeddings = compute_embeddings(files)

# %%
embedding_list = list(embeddings.values())
embedding_arr = np.array(embedding_list).reshape(-1, 384)

# %%
embedding_arr

# %%
# prompt: generate a 2d dimentional reduction of the embeddings with TSNE

from sklearn.manifold import TSNE

# Reduce dimensionality of the embeddings
tsne = TSNE(n_components=2, perplexity=30)
tsne_embeddings = tsne.fit_transform(embedding_arr)


# %%
# prompt: install umap and reduce dimension with umap
import numpy as np
import umap

# Initialize UMAP model
umap_model = umap.UMAP(n_components=2, random_state=42)

# Fit and transform the data to generate embeddings
umap_embeddings = umap_model.fit_transform(embedding_arr)


# %%
# prompt: generate embedding plot with plotly

import plotly.express as px

fig = px.scatter(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], color=values, hover_name=files)
fig.show()



# %%
fig = px.scatter(x=tsne_embeddings[:, 0], y=tsne_embeddings[:, 1], color=values, hover_name=files)
fig.show()

# %% [markdown]
# ## Train a Classification Model
# 
# The embeddings we have computed can be used as an input in a classification model. For this guide, we will be using SVM, a linear classification model.
# 
# Below, we make lists of both all of the embeddings we have computed and their associated labels. We then fit our model using those lists.

# %%
from sklearn import svm

clf = svm.SVC(gamma='scale')

y = [labels[file] for file in files]

embedding_list = list(embeddings.values())

clf.fit(np.array(embedding_list).reshape(-1, 384), y)

# %% [markdown]
# ## Classify an Image
# 
# We now have a classifier we can use to classify images!
# 
# Change the `input_file` value below to the path of a file in the `valid` or `test` directories in the image dataset with which we have been working.
# 
# Then, run the cell to classify the image.

# %%
import cv2

#any file in the folder title Now
input_file = glob.glob("Now/*.png")[0]
print(input_file)
new_image = load_image(input_file)

%matplotlib inline
#sv.plot_image(image=cv2.imread(input_file), size=(8, 8))

with torch.no_grad():
    embedding = dinov2_vits14(new_image.to(device))

    prediction = clf.predict(np.array(embedding[0].cpu()).reshape(1, -1))

    print()
    print("Predicted class: " + prediction[0])

# %%



